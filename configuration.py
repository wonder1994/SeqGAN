class generator_config(object):
    """Wrapper class for generator hyperparameter"""
    def __init__(self):
        self.emb_dim = 400  #dimension of embedding
        self.num_emb = 3092 #dimension of output unit, poem 5: 3092, character data: 47
        self.hidden_dim = 100 #dimension of hidden unit
        self.sequence_length = 20 #maximum input sequence length
        self.gen_batch_size = 64 #batch size of generator
        self.start_token = 0 #special token for start of sentence


class discriminator_config(object):
    """Wrapper class for discriminator hyperparameter"""
    def __init__(self):
        self.sequence_length = 20 #maximum input sequence length
        self.num_classes = 2 #number of class (real and fake)
        self.vocab_size = 3092 #vocabulary size, shoud be same as num_emb
        self.dis_embedding_dim = 64 #dimension of discriminator embedding space
        self.dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20] #convolutional kernel size of discriminator
        self.dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160] #number of filters of each conv. kernel
        self.dis_dropout_keep_prob = 0.75 # dropout rate of discriminator
        self.dis_l2_reg_lambda = 0.2 #L2 regularization strength
        self.dis_batch_size = 64 #Batch size for discriminator
        self.dis_learning_rate = 1e-4 #Learning rate of discriminator


class training_config(object):
    """Wrapper class for parameters for training"""
    def __init__(self):
        self.adversarial_training = False
        self.n_gram_training = not self.adversarial_training
        self.arm = True
        self.reinforce = not self.arm
        self.gen_learning_rate = 0.005 #learning rate of generator
        self.gen_update_time = 1 #update times of generator in adversarial training
        self.dis_update_time_adv = 5 #update times of discriminator in adversarial training
        self.dis_update_epoch_adv = 3 #update epoch / times of discriminator
        self.dis_update_time_pre = 20 * int(self.adversarial_training) #pretraining times of discriminator
        self.dis_update_epoch_pre = 3 #number of epoch / time in pretraining
        self.pretrained_epoch_num = 10 #Number of pretraining epoch
        self.rollout_num = 1  #Rollout number for reward estimation
        self.test_per_epoch = 3 #Test the NLL per epoch
        self.batch_size = 64 #Batch size used for training
        self.save_pretrained = 120 # Whether to save model in certain epoch (optional)
        self.grad_clip = 5.0 #Gradient Clipping
        self.seed = 88 #Random seed used for initialization
        self.start_token = 0 #special start token
        self.total_batch = 200 #total batch used for adversarial training
        self.train_file = "save/poem_train.txt"
        self.test_file = "save/poem_train.txt"
        self.positive_file = "save/real_data.txt"  # save path of real data generated by target LSTM
        self.negative_file = "save/generator_sample.txt" #save path of fake data generated by generator
        self.eval_file = "save/poem_eval_file.txt" #file used for evaluation
        self.generated_num = 1000 #Number of samples from generator used for evaluation
